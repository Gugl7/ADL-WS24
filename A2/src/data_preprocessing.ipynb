{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json \n",
    "import random\n",
    "from pathlib import Path\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from IPython.display import display\n",
    "import matplotlib\n",
    "from matplotlib import pyplot, patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sroie_folder_path = Path('../data/SROIE2019')\n",
    "example_file = Path('X51005365187.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_bbox_and_words(path: Path):\n",
    "  bbox_and_words_list = []\n",
    "\n",
    "  with open(path, 'r', errors='ignore') as f:\n",
    "    for line in f.read().splitlines():\n",
    "      if len(line) == 0:\n",
    "        continue\n",
    "        \n",
    "      split_lines = line.split(\",\")\n",
    "\n",
    "      bbox = np.array(split_lines[0:8], dtype=np.int32)\n",
    "      text = \",\".join(split_lines[8:])\n",
    "\n",
    "      # From the splited line we save (filename, [bounding box points], text line).\n",
    "      # The filename will be useful in the future\n",
    "      bbox_and_words_list.append([path.stem, *bbox, text])\n",
    "    \n",
    "  dataframe = pd.DataFrame(bbox_and_words_list, columns=['filename', 'x0', 'y0', 'x1', 'y1', 'x2', 'y2', 'x3', 'y3', 'line'])\n",
    "  dataframe = dataframe.drop(columns=['x1', 'y1', 'x3', 'y3'])\n",
    "  dataframe[['x0', 'y0', 'x2', 'y2']] = dataframe[['x0', 'y0', 'x2', 'y2']].astype(np.int16)\n",
    "\n",
    "  return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_entities(path: Path):\n",
    "  with open(path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "  dataframe = pd.DataFrame([data])\n",
    "  return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign a label to the line by checking the similarity\n",
    "# of the line and all the entities\n",
    "def assign_line_label(line: str, entities: pd.DataFrame):\n",
    "    line_set = line.replace(\",\", \"\").strip().split()\n",
    "    for i, column in enumerate(entities):\n",
    "        entity_values = entities.iloc[0, i].replace(\",\", \"\").strip()\n",
    "        entity_set = entity_values.split()\n",
    "        \n",
    "        \n",
    "        matches_count = 0\n",
    "        for l in line_set:\n",
    "            if any(SequenceMatcher(a=l, b=b).ratio() > 0.8 for b in entity_set):\n",
    "                matches_count += 1\n",
    "            \n",
    "            if (column.upper() == 'ADDRESS' and (matches_count / len(line_set)) >= 0.5) or \\\n",
    "                (column.upper() != 'ADDRESS' and (matches_count == len(line_set))) or \\\n",
    "                matches_count == len(entity_set):\n",
    "                return column.upper()\n",
    "\n",
    "    return \"O\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_labels(words: pd.DataFrame, entities: pd.DataFrame):\n",
    "    max_area = {\"TOTAL\": (0, -1), \"DATE\": (0, -1)}  # Value, index\n",
    "    already_labeled = {\"TOTAL\": False,\n",
    "                        \"DATE\": False,\n",
    "                        \"ADDRESS\": False,\n",
    "                        \"COMPANY\": False,\n",
    "                        \"O\": False\n",
    "    }\n",
    "\n",
    "    # Go through every line in $words and assign it a label\n",
    "    labels = []\n",
    "    for i, line in enumerate(words['line']):\n",
    "        label = assign_line_label(line, entities)\n",
    "\n",
    "        already_labeled[label] = True\n",
    "        if (label == \"ADDRESS\" and already_labeled[\"TOTAL\"]) or \\\n",
    "            (label == \"COMPANY\" and (already_labeled[\"DATE\"] or already_labeled[\"TOTAL\"])):\n",
    "            label = \"O\"\n",
    "\n",
    "        # Assign to the largest bounding box\n",
    "        if label in [\"TOTAL\", \"DATE\"]:\n",
    "            x0_loc = words.columns.get_loc(\"x0\")\n",
    "            bbox = words.iloc[i, x0_loc:x0_loc+4].to_list()\n",
    "            area = (bbox[2] - bbox[0]) + (bbox[3] - bbox[1])\n",
    "\n",
    "            if max_area[label][0] < area:\n",
    "                max_area[label] = (area, i)\n",
    "\n",
    "            label = \"O\"\n",
    "\n",
    "        labels.append(label)\n",
    "\n",
    "    labels[max_area[\"DATE\"][1]] = \"DATE\"\n",
    "    labels[max_area[\"TOTAL\"][1]] = \"TOTAL\"\n",
    "\n",
    "    words[\"label\"] = labels\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_line(line: pd.Series):\n",
    "  line_copy = line.copy()\n",
    "\n",
    "  line_str = line_copy.loc[\"line\"]\n",
    "  words = line_str.split(\" \")\n",
    "\n",
    "  # Filter unwanted tokens\n",
    "  words = [word for word in words if len(word) >= 1]\n",
    "\n",
    "  x0, y0, x2, y2 = line_copy.loc[['x0', 'y0', 'x2', 'y2']]\n",
    "  bbox_width = x2 - x0\n",
    "  \n",
    "\n",
    "  new_lines = []\n",
    "  for index, word in enumerate(words):\n",
    "    x2 = x0 + int(bbox_width * len(word)/len(line_str))\n",
    "    line_copy[['x0', 'x2', 'line']] = [x0, x2, word]\n",
    "    new_lines.append(line_copy.to_list())\n",
    "    x0 = x2 + 5 \n",
    "\n",
    "  return new_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_creator(folder: Path):\n",
    "  bbox_folder = folder / 'box'\n",
    "  entities_folder = folder / 'entities'\n",
    "  img_folder = folder / 'img'\n",
    "\n",
    "  # Sort by filename so that when zipping them together\n",
    "  # we don't get some other file (just in case)\n",
    "  entities_files = sorted(entities_folder.glob(\"*.txt\"))\n",
    "  bbox_files = sorted(bbox_folder.glob(\"*.txt\"))\n",
    "  img_files = sorted(img_folder.glob(\"*.jpg\"))\n",
    "\n",
    "  data = []\n",
    "\n",
    "  print(\"Reading dataset:\")\n",
    "  for bbox_file, entities_file, img_file in tqdm(zip(bbox_files, entities_files, img_files), total=len(bbox_files)):            \n",
    "    # Read the files\n",
    "    bbox = read_bbox_and_words(bbox_file)\n",
    "    entities = read_entities(entities_file)\n",
    "    image = Image.open(img_file)\n",
    "\n",
    "    # Assign labels to lines in bbox using entities\n",
    "    bbox_labeled = assign_labels(bbox, entities)\n",
    "    del bbox\n",
    "\n",
    "    # Split lines into separate tokens\n",
    "    new_bbox_l = []\n",
    "    for index, row in bbox_labeled.iterrows():\n",
    "      new_bbox_l += split_line(row)\n",
    "    new_bbox = pd.DataFrame(new_bbox_l, columns=bbox_labeled.columns)\n",
    "    del bbox_labeled\n",
    "\n",
    "\n",
    "    # Do another label assignment to keep the labeling more precise \n",
    "    for index, row in new_bbox.iterrows():\n",
    "      label = row['label']\n",
    "\n",
    "      if label != \"O\":\n",
    "        entity_values = entities.iloc[0, entities.columns.get_loc(label.lower())]\n",
    "        entity_set = entity_values.split()\n",
    "        \n",
    "        if any(SequenceMatcher(a=row['line'], b=b).ratio() > 0.7 for b in entity_set):\n",
    "            label = \"S-\" + label\n",
    "        else:\n",
    "            label = \"O\"\n",
    "      \n",
    "      new_bbox.at[index, 'label'] = label\n",
    "\n",
    "    width, height = image.size\n",
    "  \n",
    "    data.append([new_bbox, width, height])\n",
    "\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading dataset:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/626 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 517/626 [00:26<00:06, 16.77it/s]"
     ]
    }
   ],
   "source": [
    "dataset_train = dataset_creator(sroie_folder_path / 'train')\n",
    "dataset_test = dataset_creator(sroie_folder_path / 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(points: list, width: int, height: int) -> list:\n",
    "  x0, y0, x2, y2 = [int(p) for p in points]\n",
    "  \n",
    "  x0 = int(1000 * (x0 / width))\n",
    "  x2 = int(1000 * (x2 / width))\n",
    "  y0 = int(1000 * (y0 / height))\n",
    "  y2 = int(1000 * (y2 / height))\n",
    "\n",
    "  return [x0, y0, x2, y2]\n",
    "\n",
    "\n",
    "def write_dataset(dataset: list, output_dir: Path, name: str):\n",
    "  print(f\"Writing {name}ing dataset:\")\n",
    "  with open(output_dir / f\"{name}.txt\", \"w+\", encoding=\"utf8\") as file, \\\n",
    "       open(output_dir / f\"{name}_box.txt\", \"w+\", encoding=\"utf8\") as file_bbox, \\\n",
    "       open(output_dir / f\"{name}_image.txt\", \"w+\", encoding=\"utf8\") as file_image:\n",
    "\n",
    "      # Go through each dataset\n",
    "      for datas in tqdm(dataset, total=len(dataset)):\n",
    "        data, width, height = datas\n",
    "        \n",
    "        filename = data.iloc[0, data.columns.get_loc('filename')]\n",
    "\n",
    "        # Go through every row in dataset\n",
    "        for index, row in data.iterrows():\n",
    "          bbox = [int(p) for p in row[['x0', 'y0', 'x2', 'y2']]]\n",
    "          normalized_bbox = normalize(bbox, width, height)\n",
    "\n",
    "          file.write(\"{}\\t{}\\n\".format(row['line'], row['label']))\n",
    "          file_bbox.write(\"{}\\t{} {} {} {}\\n\".format(row['line'], *normalized_bbox))\n",
    "          file_image.write(\"{}\\t{} {} {} {}\\t{} {}\\t{}\\n\".format(row['line'], *bbox, width, height, filename))\n",
    "\n",
    "        # Write a second newline to separate dataset from others\n",
    "        file.write(\"\\n\")\n",
    "        file_bbox.write(\"\\n\")\n",
    "        file_image.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing training dataset:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 626/626 [00:15<00:00, 41.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing testing dataset:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 347/347 [00:08<00:00, 42.34it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_directory = Path('../data/SROIE2019')\n",
    "\n",
    "dataset_directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "write_dataset(dataset_train, dataset_directory, 'train')\n",
    "write_dataset(dataset_test, dataset_directory, 'test')\n",
    "\n",
    "\n",
    "\n",
    "# Creating the 'labels.txt' file to the the model what categories to predict.\n",
    "labels = ['COMPANY', 'DATE', 'ADDRESS', 'TOTAL']\n",
    "IOB_tags = ['S']\n",
    "with open(dataset_directory / 'labels.txt', 'w') as f:\n",
    "  for tag in IOB_tags:\n",
    "    for label in labels:\n",
    "      f.write(f\"{tag}-{label}\\n\")\n",
    "  # Writes in the last label O - meant for all non labeled words\n",
    "  f.write(\"O\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
